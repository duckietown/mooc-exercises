{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') # so we can do .to(device)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration is a method to solve for the optimal policy in Markov Decision Process by iteratively updating the value of each state. The algorithm for value iteration is shown below:\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/value_iteration_algo.png\", width = 450>\n",
    "  <figcaption>Source: Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "In this example, we will use the cartpole environment from OpenAI gym. The states of this environment are cartpole position $x$, cartpole velocity $\\dot{x}$, pole angle $\\theta$, and pole angular velocity $\\dot{\\theta}$. \n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/cartpole.png\", width = 300>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "The episode is done when:\n",
    "- $x$ < -env.x_threshold or $x$ > env.x_threshold\n",
    "- $\\theta$ < -env.theta_threshold_radians or $\\theta$ > env.theta_threshold_radians\n",
    "\n",
    "The reward is 0 when the episode is done, otherwise the rewards is +1. In other words, we want the agent to keep the cartpole and pole within the range of desired states as long as possible.\n",
    "\n",
    "We're not actually going to run this until it converges because it will take too much time. However, it is still interesting to see how one can implement value iteration to solve this environment so we can understand and appreciate the challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedCartpoleEnv(gym.envs.classic_control.CartPoleEnv):\n",
    "    def __init__(self, render_mode):\n",
    "        super().__init__(render_mode)\n",
    "    def reset(self, state=None):\n",
    "        \"\"\"\n",
    "        Modify reset function so we can teleport to any particular states if we want to.\n",
    "        \"\"\"\n",
    "        if state is not None:\n",
    "            self.state = state\n",
    "        else:\n",
    "            self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ModifiedCartpoleEnv(render_mode=\"rgb_array\")\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration requires us to compute the value of all $s \\in S$. Practically, we need to discretize the state and action space to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of possible states and range for each states\n",
    "n_x, x_min, x_max = 5, -env.x_threshold, env.x_threshold\n",
    "n_x_dot, x_dot_min, x_dot_max = 5, -8.0, 8.0 # actually from -inf to +inf\n",
    "n_theta, theta_min, theta_max = 5, -env.theta_threshold_radians, env.theta_threshold_radians\n",
    "n_theta_dot, theta_dot_min, theta_dot_max = 5, -10.0, 10.0 # actually from -inf to +inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States\n",
    "x_range = np.linspace(x_min, x_max, num = n_x)\n",
    "x_dot_range = np.linspace(x_dot_min, x_dot_max, num = n_x_dot)\n",
    "theta_range = np.linspace(theta_min, theta_max, num = n_theta)\n",
    "theta_dot_range = np.linspace(theta_dot_min, theta_dot_max, num = n_theta_dot)\n",
    "\n",
    "# Actions\n",
    "u_range = (0,1)\n",
    "\n",
    "# Create meshgrid\n",
    "x_v, x_dot_v, theta_v, theta_dot_v = np.meshgrid(x_range, x_dot_range, theta_range, theta_dot_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of list with all possible states\n",
    "states = []\n",
    "for i in range(n_x):\n",
    "    for j in range(n_x_dot):\n",
    "        for k in range(n_theta_dot):\n",
    "            for l in range(n_theta):\n",
    "                s = np.array([x_v[i,j,k,l], x_dot_v[i,j,k,l], theta_v[i,j,k,l], theta_dot_v[i,j,k,l]])\n",
    "                states.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_state(state):\n",
    "    \"\"\"\n",
    "    Function to discretize state into the range we specified above.\n",
    "    NOTE: (x_range, x_dot_range, theta_range, theta_dot_range) are defined globally in this notebook.\n",
    "    \"\"\"\n",
    "    \n",
    "    x = state[0]\n",
    "    x_dot = state[1]\n",
    "    theta = state[2]\n",
    "    theta_dot = state[3]\n",
    "    discretized_x = x_range[np.argmin(abs(x - x_range))] \n",
    "    discretized_x_dot = x_dot_range[np.argmin(abs(x_dot - x_dot_range))]\n",
    "    discretized_theta = theta_range[np.argmin(abs(theta - theta_range))]\n",
    "    discretized_theta_dot = theta_dot_range[np.argmin(abs(theta_dot - theta_dot_range))]\n",
    "    discretized_state = np.array([discretized_x, \n",
    "                                  discretized_x_dot, \n",
    "                                  discretized_theta, \n",
    "                                  discretized_theta_dot])\n",
    "    return discretized_state\n",
    "\n",
    "def get_state_index(states, discretized_state):\n",
    "    \"\"\"\n",
    "    Function to map the discretized_state onto its corresponding index in states.\n",
    "    \"\"\"\n",
    "    \n",
    "    diff = states - discretized_state\n",
    "    flags = np.any(diff, axis=1)\n",
    "    idx = np.where(flags==False)[0][0]\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gym/envs/classic_control/cartpole.py:177: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned terminated = True. You should always call 'reset()' once you receive 'terminated = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iter:  10  --- delta:  0.9135172474836413\n",
      "num_iter:  20  --- delta:  0.826168623835585\n",
      "num_iter:  30  --- delta:  0.7471720943315958\n",
      "num_iter:  40  --- delta:  0.6757290490602799\n",
      "num_iter:  50  --- delta:  0.6111172395328666\n",
      "num_iter:  60  --- delta:  0.5526834771623825\n",
      "num_iter:  70  --- delta:  0.4998370298991972\n",
      "num_iter:  80  --- delta:  0.4520436502664751\n",
      "num_iter:  90  --- delta:  0.4088201744225515\n",
      "num_iter:  100  --- delta:  0.36972963764972633\n",
      "num_iter:  110  --- delta:  0.3343768568899179\n",
      "num_iter:  120  --- delta:  0.3024044356690183\n",
      "num_iter:  130  --- delta:  0.2734891510222184\n",
      "num_iter:  140  --- delta:  0.24733868589386532\n",
      "num_iter:  150  --- delta:  0.22368867397864278\n",
      "num_iter:  160  --- delta:  0.20230002712877138\n",
      "num_iter:  170  --- delta:  0.18295651830905513\n",
      "num_iter:  180  --- delta:  0.1654625956647351\n",
      "num_iter:  190  --- delta:  0.14964140560361727\n",
      "num_iter:  200  --- delta:  0.13533300490702516\n",
      "num_iter:  210  --- delta:  0.12239274379500387\n",
      "num_iter:  220  --- delta:  0.11068980359934244\n",
      "num_iter:  230  --- delta:  0.10010587426148732\n",
      "num_iter:  240  --- delta:  0.09053395828517807\n",
      "num_iter:  250  --- delta:  0.08187728905271285\n",
      "num_iter:  260  --- delta:  0.07404835256959075\n",
      "num_iter:  270  --- delta:  0.06696800274787051\n",
      "num_iter:  280  --- delta:  0.060564661284303156\n",
      "num_iter:  290  --- delta:  0.0547735940445051\n",
      "num_iter:  300  --- delta:  0.04953625663766559\n",
      "num_iter:  310  --- delta:  0.04479970256613797\n",
      "num_iter:  320  --- delta:  0.040516047966534074\n",
      "num_iter:  330  --- delta:  0.036641987531140785\n",
      "num_iter:  340  --- delta:  0.033138356715852524\n",
      "num_iter:  350  --- delta:  0.02996973580906115\n",
      "num_iter:  360  --- delta:  0.027104091858475954\n",
      "num_iter:  370  --- delta:  0.02451245483619857\n",
      "num_iter:  380  --- delta:  0.022168624768312384\n",
      "num_iter:  390  --- delta:  0.020048906868055383\n",
      "num_iter:  400  --- delta:  0.018131871994995663\n",
      "num_iter:  410  --- delta:  0.016398140018623053\n",
      "num_iter:  420  --- delta:  0.01483018389633628\n",
      "num_iter:  430  --- delta:  0.013412152484931994\n",
      "num_iter:  440  --- delta:  0.01212971029465848\n",
      "num_iter:  450  --- delta:  0.010969892565526607\n",
      "num_iter:  460  --- delta:  0.009920974201037325\n",
      "Value iteration converged after 460 iterations\n"
     ]
    }
   ],
   "source": [
    "convergence_threshold = 0.01\n",
    "delta = 100000\n",
    "gamma = 0.99\n",
    "deltas = []\n",
    "V = np.zeros(n_x * n_x_dot * n_theta * n_theta_dot) # initialize V(s) = 0 for all states\n",
    "\n",
    "num_iter = 0\n",
    "while delta > convergence_threshold:\n",
    "    delta = 0\n",
    "    for i in range(len(states)): # for each possible state\n",
    "        state = states[i]\n",
    "        v = V[i] # value of current state\n",
    "        V_max = -999999999\n",
    "        for u in u_range: # evaluate all possible actions (due to V(s) <- max_a ...)\n",
    "            env.reset(state = state) # make sure we are at state = state\n",
    "            next_state, r, terminated, truncated, _  = env.step(u) # apply u for one step\n",
    "            discretized_next_state = discretize_state(next_state)\n",
    "            next_state_index = get_state_index(states, discretized_next_state)\n",
    "            value = r + gamma * V[next_state_index]\n",
    "            if value > V_max:\n",
    "                V_max = value\n",
    "        V[i] = V_max # update value of current state\n",
    "        delta = max(delta, abs(v - V_max)) # update delta for convergence check\n",
    "    deltas.append(delta)\n",
    "    num_iter+=1\n",
    "    if num_iter % 10 == 0:\n",
    "        print('num_iter: ', num_iter, ' --- delta: ', delta)\n",
    "print('Value iteration converged after %d iterations' % num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(deltas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to determine how we choose an action based on $V(s)$. For example, we can do this greedily by doing:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_a \\sum_{s',r} p(s',r|s,a)[r + \\gamma V(s')] \n",
    "$$\n",
    "\n",
    "So, let's create another lookup table for the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_policy(V):\n",
    "    \"\"\"\n",
    "    Function to return the optimal policy from a given V.\n",
    "    NOTE: (states, n_x, n_x_dot, n_theta_dot, n_theta, gamma) are defined globally in this notebook.\n",
    "    \"\"\"\n",
    "    policy = np.zeros(n_x * n_x_dot * n_theta_dot * n_theta)\n",
    "    for i in range(len(V)):\n",
    "        state = states[i]\n",
    "        V_max = -999999999\n",
    "        u_max = -999999999\n",
    "        for u in u_range: # evaluate all action\n",
    "            env.reset(state = state) # make sure we are at state = state\n",
    "            next_state, r, terminated, truncated, _  = env.step(u) # apply u for one step\n",
    "            discretized_next_state = discretize_state(next_state)\n",
    "            next_state_index = get_state_index(states, discretized_next_state)\n",
    "            value = r + gamma * V[next_state_index]\n",
    "            if value > V_max:\n",
    "                V_max = value\n",
    "                u_max = u\n",
    "        policy[i] = u_max\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = get_best_policy(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the performance of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, policy):\n",
    "    \"\"\"\n",
    "    Function to get what action to take according to the policy.\n",
    "    NOTE: (states) is defined globally in this notebook.\n",
    "    \"\"\"\n",
    "    discretized_state = discretize_state(state)\n",
    "    state_index = get_state_index(states, discretized_state)\n",
    "    u = policy[state_index]\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "while not (terminated or truncated):\n",
    "    action = get_action(state, policy)\n",
    "    state, reward, terminated, truncated,  _ = env.step(int(action))\n",
    "    total_reward += reward\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this example, we can identify several challenges:\n",
    "1. If discretization is not fine enough, then applying an action may not change the state at all.\n",
    "2. If discretization is too fine, then it will take a long time for the algorithm to converge.\n",
    "3. As the state and action spaces get bigger, the computation needed to do value iteration increases drastically.\n",
    "\n",
    "Now, let's take a look at REINFORCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
