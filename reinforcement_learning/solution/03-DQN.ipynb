{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') # so we can do .to(device)\n",
    "    print(\"found GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"no GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN is an off-policy Q learning method. Unlike REINFORCE, DQN learns a Q networks (as opposed to policy) and can use trajectories that the agent has collected in the past to train the current Q network by storing them in a replay buffer. The learning objective in DQN is also different than in REINFORCE: DQN tries to minimize the **Temporal Difference (TD)** error (i.e., difference between prediction of Q values and the TD target). We are not going to talk about TD learning in detail here, but if you are especially interested to work with RL, I recommend you to look into TD learning and SARSA to have a better understanding about DQN. The TD target for DQN is defined as:\n",
    "\n",
    "$$\n",
    "Q_{target}(s,a) = r + \\gamma \\max_{a'}Q(s',a')\n",
    "$$\n",
    "\n",
    "The DQN algorithm is shown below:\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/dqn_algo.png\", width = 600>\n",
    "  <figcaption>Source: Mnih et al. (2013). Playing Atari with Deep Reinforcement Learning.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "Note that depending on the use case, we may or may not use the feature network $\\phi$ shown in the algorithm above. For this example, we know what the state is, so we do not need to differentiate between observations $x$ and states $s$.\n",
    "\n",
    "In practice, we often use another network that is a lagged copy of the Q-network to generate the TD target in order to stabilize training. We call this a **target network**. If you are interested to know more about this, I encourage you to read the paper :)\n",
    "\n",
    "Generally, with off-policy methods, one needs to be aware of memory usage since we are storing a lot of information in the replay buffer. This can be problematic especially when the state dimension is high (e.g., images).\n",
    "\n",
    "Let's now take a look at the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size):\n",
    "        # Use deque instead of list so we do not have to manually \n",
    "        # pop the buffer when it reaches max buffer size\n",
    "        self.buffer = deque(maxlen = buffer_size) \n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Function to push the data into buffer.\n",
    "        Input:\n",
    "            - state: state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - action: int\n",
    "            - reward: float\n",
    "            - next_state: next_state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - done: bool\n",
    "        \"\"\"\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Function to sample a batch from replay buffer.\n",
    "        Input:\n",
    "            - batch_size: an int\n",
    "        \"\"\"\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        for i in range(batch_size):\n",
    "            state = samples[i][0]\n",
    "            action = samples[i][1]\n",
    "            reward = samples[i][2]\n",
    "            next_state = samples[i][3]\n",
    "            done = samples[i][4]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards), np.stack(next_states), np.stack(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = F.relu(self.dropout(self.fc1(inp)))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        \n",
    "        # DQN parameters\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.buffer_size = 1000\n",
    "        self.epsilon_init = 1.0\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 200\n",
    "        self.gamma = 0.99\n",
    "        self.log_interval = 250 # print progress per this many episodes\n",
    "        self.target_update_freq = 10 # update target network per this many episodes\n",
    "        \n",
    "        # Models\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict()) # copy of q_network\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.q_network_optimizer = optim.Adam(self.q_network.parameters(), lr = 1e-2)\n",
    "        self.q_network.train()\n",
    "        self.target_network.eval() # We never train the network we use to generate the TD target!\n",
    "\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # [1, dim_space]\n",
    "        # Using epsilon greedy as our policy\n",
    "        if np.random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_value = self.q_network(state) # [1, action_space]\n",
    "                action  = q_value.max(1) # returns both the max values and max index\n",
    "                action = action[1].data[0] # [1] indicates we want the max index\n",
    "                action = action.item()\n",
    "        else: # With probability epsilon, select random action\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Convert ndarray to torch tensor\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        dones = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        # Calculate TD error\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1) # [batch_size]\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach() # [0] indicates we want the max values (not the indices!), detach since this is the target # [batch_size]\n",
    "        td_target = rewards + self.gamma * next_q_values * (1. - dones) \n",
    "        loss = F.mse_loss(q_values, td_target)\n",
    "        \n",
    "        # Update model\n",
    "        self.q_network_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.q_network_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4\n",
    "action_dim = 2\n",
    "agent = DQNAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250 \t Running Reward: 12.74\n",
      "Episode 500 \t Running Reward: 13.28\n",
      "Episode 750 \t Running Reward: 13.72\n",
      "Episode 1000 \t Running Reward: 18.34\n",
      "Episode 1250 \t Running Reward: 21.91\n",
      "Episode 1500 \t Running Reward: 21.78\n",
      "Episode 1750 \t Running Reward: 25.33\n",
      "Episode 2000 \t Running Reward: 30.51\n",
      "Episode 2250 \t Running Reward: 33.65\n",
      "Episode 2500 \t Running Reward: 30.81\n",
      "Episode 2750 \t Running Reward: 29.64\n",
      "Episode 3000 \t Running Reward: 32.18\n",
      "Episode 3250 \t Running Reward: 29.89\n",
      "Episode 3500 \t Running Reward: 35.22\n",
      "Episode 3750 \t Running Reward: 29.41\n",
      "Episode 4000 \t Running Reward: 36.08\n",
      "Episode 4250 \t Running Reward: 41.93\n",
      "Episode 4500 \t Running Reward: 43.53\n",
      "Episode 4750 \t Running Reward: 34.16\n",
      "Episode 5000 \t Running Reward: 35.47\n",
      "Episode 5250 \t Running Reward: 33.73\n",
      "Episode 5500 \t Running Reward: 40.96\n",
      "Episode 5750 \t Running Reward: 44.13\n",
      "Episode 6000 \t Running Reward: 49.02\n",
      "Episode 6250 \t Running Reward: 44.36\n",
      "Episode 6500 \t Running Reward: 40.88\n",
      "Episode 6750 \t Running Reward: 41.85\n",
      "Episode 7000 \t Running Reward: 46.01\n",
      "Episode 7250 \t Running Reward: 47.03\n",
      "Episode 7500 \t Running Reward: 49.45\n",
      "Episode 7750 \t Running Reward: 46.07\n",
      "Episode 8000 \t Running Reward: 50.56\n",
      "Episode 8250 \t Running Reward: 47.87\n",
      "Episode 8500 \t Running Reward: 42.72\n",
      "Episode 8750 \t Running Reward: 43.01\n",
      "Episode 9000 \t Running Reward: 45.90\n",
      "Episode 9250 \t Running Reward: 49.36\n",
      "Episode 9500 \t Running Reward: 42.62\n",
      "Episode 9750 \t Running Reward: 45.84\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "running_reward = 10\n",
    "target_update_freq = 10 # update target network every X episode\n",
    "episode = 1 # indicate episode number\n",
    "\n",
    "state, ep_reward = env.reset(), 0\n",
    "state = state[0]\n",
    "for i in range(1, 100000):\n",
    "    \n",
    "    # Update epsilon and pick action\n",
    "    epsilon = agent.epsilon_end + (agent.epsilon_init - agent.epsilon_end) * np.exp(-1. * i / agent.epsilon_decay)\n",
    "    action = agent.get_action(state, epsilon)\n",
    "\n",
    "    # Take a step\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    # Update replay buffer\n",
    "    agent.replay_buffer.update(state, action, reward, next_state, terminated)\n",
    "\n",
    "    # Once replay buffer size is larger than batch size, start training\n",
    "    if len(agent.replay_buffer.buffer) > batch_size:\n",
    "        agent.update(batch_size)\n",
    "\n",
    "    # Update episode reward and check for end episode\n",
    "    ep_reward += reward\n",
    "    if terminated or truncated: # If episode is done: update running reward, reset env, reset episode reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        episode += 1 # increment episode count (used for updating target network)\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        state = state[0]\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    # Occasionally, update the target_network by copying the q_network\n",
    "    if episode % agent.target_update_freq == 0:\n",
    "        agent.target_network.load_state_dict(agent.q_network.state_dict())\n",
    "        \n",
    "    if i % agent.log_interval == 0:\n",
    "        print('Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "    \n",
    "    # Stopping criteria\n",
    "    if running_reward > 100:\n",
    "        print('Solved: Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = state[0]\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "while not (terminated or truncated):\n",
    "    action = agent.get_action(state, 0.0)\n",
    "    state, reward, terminated, truncated,  _ = env.step(int(action))\n",
    "    total_reward += reward\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
