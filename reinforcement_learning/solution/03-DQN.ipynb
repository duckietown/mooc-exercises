{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') # so we can do .to(device)\n",
    "    print(\"found GPU\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"no GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN is an off-policy Q learning method. Unlike REINFORCE, DQN learns a Q networks (as opposed to policy) and can use trajectories that the agent has collected in the past to train the current Q network by storing them in a replay buffer. The learning objective in DQN is also different than in REINFORCE: DQN tries to minimize the **Temporal Difference (TD)** error (i.e., difference between prediction of Q values and the TD target). We are not going to talk about TD learning in detail here, but if you are especially interested to work with RL, I recommend you to look into TD learning and SARSA to have a better understanding about DQN. The TD target for DQN is defined as:\n",
    "\n",
    "$$\n",
    "Q_{target}(s,a) = r + \\gamma \\max_{a'}Q(s',a')\n",
    "$$\n",
    "\n",
    "The DQN algorithm is shown below:\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/dqn_algo.png\", width = 600>\n",
    "  <figcaption>Source: Mnih et al. (2013). Playing Atari with Deep Reinforcement Learning.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "Note that depending on the use case, we may or may not use the feature network $\\phi$ shown in the algorithm above. For this example, we know what the state is, so we do not need to differentiate between observations $x$ and states $s$.\n",
    "\n",
    "In practice, we often use another network that is a lagged copy of the Q-network to generate the TD target in order to stabilize training. We call this a **target network**. If you are interested to know more about this, I encourage you to read the paper :)\n",
    "\n",
    "Generally, with off-policy methods, one needs to be aware of memory usage since we are storing a lot of information in the replay buffer. This can be problematic especially when the state dimension is high (e.g., images).\n",
    "\n",
    "Let's now take a look at the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size):\n",
    "        # Use deque instead of list so we do not have to manually \n",
    "        # pop the buffer when it reaches max buffer size\n",
    "        self.buffer = deque(maxlen = buffer_size) \n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Function to push the data into buffer.\n",
    "        Input:\n",
    "            - state: state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - action: int\n",
    "            - reward: float\n",
    "            - next_state: next_state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - done: bool\n",
    "        \"\"\"\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Function to sample a batch from replay buffer.\n",
    "        Input:\n",
    "            - batch_size: an int\n",
    "        \"\"\"\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        for i in range(batch_size):\n",
    "            state = samples[i][0]\n",
    "            action = samples[i][1]\n",
    "            reward = samples[i][2]\n",
    "            next_state = samples[i][3]\n",
    "            done = samples[i][4]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards), np.stack(next_states), np.stack(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = F.relu(self.dropout(self.fc1(inp)))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        \n",
    "        # DQN parameters\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.buffer_size = 1000\n",
    "        self.epsilon_init = 1.0\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 200\n",
    "        self.gamma = 0.99\n",
    "        self.log_interval = 250 # print progress per this many episodes\n",
    "        self.target_update_freq = 10 # update target network per this many episodes\n",
    "        \n",
    "        # Models\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict()) # copy of q_network\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.q_network_optimizer = optim.Adam(self.q_network.parameters(), lr = 1e-2)\n",
    "        self.q_network.train()\n",
    "        self.target_network.eval() # We never train the network we use to generate the TD target!\n",
    "\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # [1, dim_space]\n",
    "        # Using epsilon greedy as our policy\n",
    "        if np.random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_value = self.q_network(state) # [1, action_space]\n",
    "                action  = q_value.max(1) # returns both the max values and max index\n",
    "                action = action[1].data[0] # [1] indicates we want the max index\n",
    "                action = action.item()\n",
    "        else: # With probability epsilon, select random action\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Convert ndarray to torch tensor\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        dones = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        # Calculate TD error\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1) # [batch_size]\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach() # [0] indicates we want the max values (not the indices!), detach since this is the target # [batch_size]\n",
    "        td_target = rewards + self.gamma * next_q_values * (1. - dones) \n",
    "        loss = F.mse_loss(q_values, td_target)\n",
    "        \n",
    "        # Update model\n",
    "        self.q_network_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.q_network_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4\n",
    "action_dim = 2\n",
    "agent = DQNAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250 \t Running Reward: 12.87\n",
      "Episode 500 \t Running Reward: 15.28\n",
      "Episode 750 \t Running Reward: 20.35\n",
      "Episode 1000 \t Running Reward: 16.19\n",
      "Episode 1250 \t Running Reward: 23.40\n",
      "Episode 1500 \t Running Reward: 29.15\n",
      "Episode 1750 \t Running Reward: 21.15\n",
      "Episode 2000 \t Running Reward: 23.19\n",
      "Episode 2250 \t Running Reward: 27.42\n",
      "Episode 2500 \t Running Reward: 31.34\n",
      "Episode 2750 \t Running Reward: 32.32\n",
      "Episode 3000 \t Running Reward: 33.22\n",
      "Episode 3250 \t Running Reward: 37.82\n",
      "Episode 3500 \t Running Reward: 43.13\n",
      "Episode 3750 \t Running Reward: 48.07\n",
      "Episode 4000 \t Running Reward: 25.89\n",
      "Episode 4250 \t Running Reward: 29.73\n",
      "Episode 4500 \t Running Reward: 39.52\n",
      "Episode 4750 \t Running Reward: 45.87\n",
      "Episode 5000 \t Running Reward: 51.37\n",
      "Episode 5250 \t Running Reward: 56.81\n",
      "Episode 5500 \t Running Reward: 61.26\n",
      "Episode 5750 \t Running Reward: 57.38\n",
      "Episode 6000 \t Running Reward: 50.85\n",
      "Episode 6250 \t Running Reward: 43.91\n",
      "Episode 6500 \t Running Reward: 44.97\n",
      "Episode 6750 \t Running Reward: 47.52\n",
      "Episode 7000 \t Running Reward: 52.19\n",
      "Episode 7250 \t Running Reward: 44.21\n",
      "Episode 7500 \t Running Reward: 46.54\n",
      "Episode 7750 \t Running Reward: 45.12\n",
      "Episode 8000 \t Running Reward: 50.71\n",
      "Episode 8250 \t Running Reward: 52.42\n",
      "Episode 8500 \t Running Reward: 49.82\n",
      "Episode 8750 \t Running Reward: 54.23\n",
      "Episode 9000 \t Running Reward: 64.60\n",
      "Episode 9250 \t Running Reward: 70.52\n",
      "Episode 9500 \t Running Reward: 72.64\n",
      "Episode 9750 \t Running Reward: 76.25\n",
      "Episode 10000 \t Running Reward: 83.58\n",
      "Episode 10250 \t Running Reward: 87.32\n",
      "Episode 10500 \t Running Reward: 86.08\n",
      "Episode 10750 \t Running Reward: 79.10\n",
      "Episode 11000 \t Running Reward: 75.72\n",
      "Episode 11250 \t Running Reward: 70.60\n",
      "Episode 11500 \t Running Reward: 71.49\n",
      "Episode 11750 \t Running Reward: 69.60\n",
      "Episode 12000 \t Running Reward: 66.10\n",
      "Episode 12250 \t Running Reward: 71.13\n",
      "Episode 12500 \t Running Reward: 70.12\n",
      "Episode 12750 \t Running Reward: 57.63\n",
      "Episode 13000 \t Running Reward: 60.69\n",
      "Episode 13250 \t Running Reward: 65.43\n",
      "Episode 13500 \t Running Reward: 65.72\n",
      "Episode 13750 \t Running Reward: 66.64\n",
      "Episode 14000 \t Running Reward: 66.82\n",
      "Episode 14250 \t Running Reward: 71.43\n",
      "Episode 14500 \t Running Reward: 73.06\n",
      "Episode 14750 \t Running Reward: 75.37\n",
      "Episode 15000 \t Running Reward: 77.36\n",
      "Episode 15250 \t Running Reward: 78.42\n",
      "Episode 15500 \t Running Reward: 76.31\n",
      "Episode 15750 \t Running Reward: 84.23\n",
      "Episode 16000 \t Running Reward: 89.87\n",
      "Episode 16250 \t Running Reward: 84.72\n",
      "Episode 16500 \t Running Reward: 91.23\n",
      "Episode 16750 \t Running Reward: 66.31\n",
      "Episode 17000 \t Running Reward: 65.61\n",
      "Episode 17250 \t Running Reward: 68.63\n",
      "Episode 17500 \t Running Reward: 67.62\n",
      "Episode 17750 \t Running Reward: 69.94\n",
      "Episode 18000 \t Running Reward: 70.61\n",
      "Episode 18250 \t Running Reward: 75.05\n",
      "Episode 18500 \t Running Reward: 74.31\n",
      "Episode 18750 \t Running Reward: 77.49\n",
      "Episode 19000 \t Running Reward: 74.99\n",
      "Episode 19250 \t Running Reward: 74.99\n",
      "Episode 19500 \t Running Reward: 79.39\n",
      "Episode 19750 \t Running Reward: 76.84\n",
      "Episode 20000 \t Running Reward: 81.41\n",
      "Episode 20250 \t Running Reward: 76.83\n",
      "Episode 20500 \t Running Reward: 68.45\n",
      "Episode 20750 \t Running Reward: 68.51\n",
      "Episode 21000 \t Running Reward: 67.95\n",
      "Episode 21250 \t Running Reward: 66.44\n",
      "Episode 21500 \t Running Reward: 67.26\n",
      "Episode 21750 \t Running Reward: 71.11\n",
      "Episode 22000 \t Running Reward: 60.77\n",
      "Episode 22250 \t Running Reward: 66.93\n",
      "Episode 22500 \t Running Reward: 62.55\n",
      "Episode 22750 \t Running Reward: 68.62\n",
      "Episode 23000 \t Running Reward: 74.51\n",
      "Episode 23250 \t Running Reward: 76.87\n",
      "Episode 23500 \t Running Reward: 83.70\n",
      "Episode 23750 \t Running Reward: 75.41\n",
      "Episode 24000 \t Running Reward: 77.74\n",
      "Episode 24250 \t Running Reward: 77.73\n",
      "Episode 24500 \t Running Reward: 78.84\n",
      "Episode 24750 \t Running Reward: 68.62\n",
      "Episode 25000 \t Running Reward: 46.74\n",
      "Episode 25250 \t Running Reward: 40.17\n",
      "Episode 25500 \t Running Reward: 35.75\n",
      "Episode 25750 \t Running Reward: 39.40\n",
      "Episode 26000 \t Running Reward: 36.59\n",
      "Episode 26250 \t Running Reward: 38.27\n",
      "Episode 26500 \t Running Reward: 44.88\n",
      "Episode 26750 \t Running Reward: 45.96\n",
      "Episode 27000 \t Running Reward: 49.06\n",
      "Episode 27250 \t Running Reward: 54.31\n",
      "Episode 27500 \t Running Reward: 62.64\n",
      "Episode 27750 \t Running Reward: 69.56\n",
      "Episode 28000 \t Running Reward: 70.12\n",
      "Episode 28250 \t Running Reward: 75.68\n",
      "Episode 28500 \t Running Reward: 69.59\n",
      "Episode 28750 \t Running Reward: 63.11\n",
      "Episode 29000 \t Running Reward: 65.43\n",
      "Episode 29250 \t Running Reward: 71.94\n",
      "Episode 29500 \t Running Reward: 75.83\n",
      "Episode 29750 \t Running Reward: 79.68\n",
      "Episode 30000 \t Running Reward: 87.46\n",
      "Episode 30250 \t Running Reward: 82.51\n",
      "Episode 30500 \t Running Reward: 61.38\n",
      "Episode 30750 \t Running Reward: 57.57\n",
      "Episode 31000 \t Running Reward: 52.37\n",
      "Episode 31250 \t Running Reward: 54.40\n",
      "Episode 31500 \t Running Reward: 42.29\n",
      "Episode 31750 \t Running Reward: 43.51\n",
      "Episode 32000 \t Running Reward: 49.62\n",
      "Episode 32250 \t Running Reward: 56.21\n",
      "Episode 32500 \t Running Reward: 61.96\n",
      "Episode 32750 \t Running Reward: 67.99\n",
      "Episode 33000 \t Running Reward: 73.10\n",
      "Episode 33250 \t Running Reward: 77.40\n",
      "Episode 33500 \t Running Reward: 80.36\n",
      "Episode 33750 \t Running Reward: 71.69\n",
      "Episode 34000 \t Running Reward: 65.94\n",
      "Episode 34250 \t Running Reward: 71.69\n",
      "Episode 34500 \t Running Reward: 70.93\n",
      "Episode 34750 \t Running Reward: 73.08\n",
      "Episode 35000 \t Running Reward: 75.05\n",
      "Episode 35250 \t Running Reward: 80.21\n",
      "Episode 35500 \t Running Reward: 75.80\n",
      "Episode 35750 \t Running Reward: 75.83\n",
      "Episode 36000 \t Running Reward: 81.95\n",
      "Episode 36250 \t Running Reward: 82.34\n",
      "Episode 36500 \t Running Reward: 82.44\n",
      "Episode 36750 \t Running Reward: 85.90\n",
      "Episode 37000 \t Running Reward: 82.75\n",
      "Episode 37250 \t Running Reward: 89.91\n",
      "Episode 37500 \t Running Reward: 84.76\n",
      "Episode 37750 \t Running Reward: 71.77\n",
      "Episode 38000 \t Running Reward: 65.35\n",
      "Episode 38250 \t Running Reward: 64.41\n",
      "Episode 38500 \t Running Reward: 65.09\n",
      "Episode 38750 \t Running Reward: 63.44\n",
      "Episode 39000 \t Running Reward: 67.08\n",
      "Episode 39250 \t Running Reward: 66.07\n",
      "Episode 39500 \t Running Reward: 68.54\n",
      "Episode 39750 \t Running Reward: 78.37\n",
      "Episode 40000 \t Running Reward: 81.15\n",
      "Episode 40250 \t Running Reward: 84.55\n",
      "Episode 40500 \t Running Reward: 92.57\n",
      "Episode 40750 \t Running Reward: 91.70\n",
      "Solved: Episode 40955 \t Running Reward: 102.71\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "running_reward = 10\n",
    "target_update_freq = 10 # update target network every X episode\n",
    "episode = 1 # indicate episode number\n",
    "\n",
    "state, ep_reward = env.reset(), 0\n",
    "state = state[0]\n",
    "for i in range(1, 100000):\n",
    "    \n",
    "    # Update epsilon and pick action\n",
    "    epsilon = agent.epsilon_end + (agent.epsilon_init - agent.epsilon_end) * np.exp(-1. * i / agent.epsilon_decay)\n",
    "    action = agent.get_action(state, epsilon)\n",
    "\n",
    "    # Take a step\n",
    "    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    # Update replay buffer\n",
    "    agent.replay_buffer.update(state, action, reward, next_state, terminated)\n",
    "\n",
    "    # Once replay buffer size is larger than batch size, start training\n",
    "    if len(agent.replay_buffer.buffer) > batch_size:\n",
    "        agent.update(batch_size)\n",
    "\n",
    "    # Update episode reward and check for end episode\n",
    "    ep_reward += reward\n",
    "    if terminated or truncated: # If episode is done: update running reward, reset env, reset episode reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        episode += 1 # increment episode count (used for updating target network)\n",
    "        state, ep_reward = env.reset(), 0\n",
    "        state = state[0]\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    # Occasionally, update the target_network by copying the q_network\n",
    "    if episode % agent.target_update_freq == 0:\n",
    "        agent.target_network.load_state_dict(agent.q_network.state_dict())\n",
    "        \n",
    "    if i % agent.log_interval == 0:\n",
    "        print('Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "    \n",
    "    # Stopping criteria\n",
    "    if running_reward > 100:\n",
    "        print('Solved: Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (terminated \u001b[38;5;129;01mor\u001b[39;00m truncated):\n\u001b[0;32m----> 7\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     state, reward, terminated, truncated,  _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;28mint\u001b[39m(action))\n\u001b[1;32m      9\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mDQNAgent.get_action\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m>\u001b[39m epsilon:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 33\u001b[0m         q_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [1, action_space]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m         action  \u001b[38;5;241m=\u001b[39m q_value\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# returns both the max values and max index\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         action \u001b[38;5;241m=\u001b[39m action[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# [1] indicates we want the max index\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, inp)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp):\n\u001b[0;32m----> 9\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     10\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasLtMatmul( ltHandle, computeDesc.descriptor(), &alpha_val, mat1_ptr, Adesc.descriptor(), mat2_ptr, Bdesc.descriptor(), &beta_val, result_ptr, Cdesc.descriptor(), result_ptr, Cdesc.descriptor(), &heuristicResult.algo, workspace.data_ptr(), workspaceSize, at::cuda::getCurrentCUDAStream())`"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "state = state[0]\n",
    "terminated = False\n",
    "truncated = False\n",
    "total_reward = 0\n",
    "while not (terminated or truncated):\n",
    "    action = agent.get_action(state, 0.0)\n",
    "    state, reward, terminated, truncated,  _ = env.step(int(action))\n",
    "    total_reward += reward\n",
    "    plt.imshow(env.render())\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
