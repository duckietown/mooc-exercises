{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m count\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') # so we can do .to(device)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:commons:version: 6.2.4 *\n",
      "DEBUG:typing:version: 6.2.3\n",
      "DEBUG:duckietown_world:duckietown-world version 6.2.39 path /usr/local/lib/python3.8/dist-packages\n",
      "DEBUG:geometry:PyGeometry-z6 version 2.1.4 path /usr/local/lib/python3.8/dist-packages\n",
      "DEBUG:aido_schemas:aido-protocols version 6.0.59 path /usr/local/lib/python3.8/dist-packages\n",
      "DEBUG:nodes:version 6.2.13 path /usr/local/lib/python3.8/dist-packages pyparsing 2.4.6\n",
      "DEBUG:gym-duckietown:gym-duckietown version 6.1.30 path /usr/local/lib/python3.8/dist-packages\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': ('xaudio2', 'directsound', 'openal', 'pulse', 'silent'), 'debug_font': False, 'debug_gl': True, 'debug_gl_trace': False, 'debug_gl_trace_args': False, 'debug_graphics_batch': False, 'debug_lib': False, 'debug_media': False, 'debug_texture': False, 'debug_trace': False, 'debug_trace_args': False, 'debug_trace_depth': 1, 'debug_trace_flush': True, 'debug_win32': False, 'debug_x11': False, 'graphics_vbo': True, 'shadow_window': True, 'vsync': None, 'xsync': True, 'xlib_fullscreen_override_redirect': False, 'darwin_cocoa': True, 'search_local_libs': True, 'headless': False, 'headless_device': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym-duckietown:Information about the graphics card:\n",
      " pyglet_version: 1.5.15\n",
      "    information: dict[4]\n",
      "                 │ \u001b[33mvendor\u001b[0m: NVIDIA Corporation\n",
      "                 │ \u001b[33mrenderer\u001b[0m: NVIDIA GeForce RTX 3070 Laptop GPU/PCIe/SSE2\n",
      "                 │ \u001b[33mversion\u001b[0m: 4.6.0 NVIDIA 470.141.03\n",
      "                 │ \u001b[33mshading-language-version\u001b[0m: 4.60 NVIDIA\n",
      "  nvidia_around: True\n",
      "INFO:duckietown_world: data: /usr/local/lib/python3.8/dist-packages/duckietown_world/data\n",
      "DEBUG:gym-duckietown:loading map file \"/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/maps/udem1.yaml\"\n",
      "INFO:gym-duckietown:done\n",
      "DEBUG:gym-duckietown:loading mesh 'duckiebot' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/duckiebot/duckiebot.obj'\n",
      "DEBUG:gym-duckietown:loading materials from /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/duckiebot/duckiebot.mtl\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/duckiebot/circlegrid-square.jpg\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/duckie/duckie.png\n",
      "DEBUG:gym-duckietown:loading mesh 'tree' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/tree/tree.obj'\n",
      "DEBUG:gym-duckietown:loading materials from /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/tree/tree.mtl\n",
      "DEBUG:gym-duckietown:loading mesh 'duckie' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/duckie/duckie.obj'\n",
      "DEBUG:gym-duckietown:loading mesh 'sign_generic' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_generic/sign_generic.obj'\n",
      "DEBUG:gym-duckietown:loading materials from /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_generic/sign_generic.mtl\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_stop/sign_stop.png\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/wood_osb.jpg\n",
      "DEBUG:gym-duckietown:loading mesh 'sign_generic' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_generic/sign_generic.obj'\n",
      "DEBUG:gym-duckietown:loading materials from /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_generic/sign_generic.mtl\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_left_T_intersect/sign_left_T_intersect.png\n",
      "DEBUG:gym-duckietown:loading mesh 'sign_generic' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_generic/sign_generic.obj'\n",
      "DEBUG:gym-duckietown:loading materials from /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_generic/sign_generic.mtl\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_right_T_intersect/sign_right_T_intersect.png\n",
      "DEBUG:gym-duckietown:loading mesh 'sign_generic' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_generic/sign_generic.obj'\n",
      "DEBUG:gym-duckietown:loading materials from /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_generic/sign_generic.mtl\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/signs/sign_T_intersect/sign_T_intersect.png\n",
      "DEBUG:gym-duckietown:loading mesh 'house' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/house/house.obj'\n",
      "DEBUG:gym-duckietown:loading materials from /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/house/house.mtl\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/house/house.png\n",
      "DEBUG:gym-duckietown:loading mesh 'truck' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/truck/truck.obj'\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/truck/truck.png\n",
      "DEBUG:gym-duckietown:loading mesh 'bus' from file_path '/usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/bus/bus.obj'\n",
      "DEBUG:gym-duckietown:loading texture: /usr/local/lib/python3.8/dist-packages/duckietown_world/data/gd1/meshes/bus/bus.png\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.random._generator.Generator' object has no attribute 'randint'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym_duckietown\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simulator\n\u001b[0;32m----> 4\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mSimulator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym_duckietown/simulator.py:381\u001b[0m, in \u001b[0;36mSimulator.__init__\u001b[0;34m(self, map_name, max_steps, draw_curve, draw_bbox, domain_rand, frame_rate, frame_skip, camera_width, camera_height, robot_speed, accept_start_angle_deg, full_transparency, user_tile_start, seed, distortion, dynamics_rand, camera_rand, randomize_maps_on_reset, num_tris_distractors, color_ground, color_sky, style, enable_leds)\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_names \u001b[38;5;241m=\u001b[39m [mapfile\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m mapfile \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_names]\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# Initialize the state\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwheelVels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym_duckietown/simulator.py:546\u001b[0m, in \u001b[0;36mSimulator.reset\u001b[0;34m(self, segment)\u001b[0m\n\u001b[1;32m    543\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandom map chosen: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmap_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_map(map_name)\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandomization_settings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandomizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandomize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnp_random\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# Horizon color\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m# Note: we explicitly sample white and grey/black because\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# these colors are easily confused for road and lane markings\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdomain_rand:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym_duckietown/randomization/randomizer.py:45\u001b[0m, in \u001b[0;36mRandomizer.randomize\u001b[0;34m(self, rng)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check your randomization definition for: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k))\n\u001b[0;32m---> 45\u001b[0m     setting \u001b[38;5;241m=\u001b[39m \u001b[43mrng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m(low\u001b[38;5;241m=\u001b[39mlow, high\u001b[38;5;241m=\u001b[39mhigh, size\u001b[38;5;241m=\u001b[39msize)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m randomization_definition[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.random._generator.Generator' object has no attribute 'randint'"
     ]
    }
   ],
   "source": [
    "from gym_duckietown.simulator import Simulator\n",
    "\n",
    "\n",
    "env = Simulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN is an off-policy Q learning method. Unlike REINFORCE, DQN learns a Q networks (as opposed to policy) and can use trajectories that the agent has collected in the past to train the current Q network by storing them in a replay buffer. The learning objective in DQN is also different than in REINFORCE: DQN tries to minimize the **Temporal Difference (TD)** error (i.e., difference between prediction of Q values and the TD target). We are not going to talk about TD learning in detail here, but if you are especially interested to work with RL, I recommend you to look into TD learning and SARSA to have a better understanding about DQN. The TD target for DQN is defined as:\n",
    "\n",
    "$$\n",
    "Q_{target}(s,a) = r + \\gamma \\max_{a'}Q(s',a')\n",
    "$$\n",
    "\n",
    "The DQN algorithm is shown below:\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/dqn_algo.png\", width = 600>\n",
    "  <figcaption>Source: Mnih et al. (2013). Playing Atari with Deep Reinforcement Learning.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "Note that depending on the use case, we may or may not use the feature network $\\phi$ shown in the algorithm above. For this example, we know what the state is, so we do not need to differentiate between observations $x$ and states $s$.\n",
    "\n",
    "In practice, we often use another network that is a lagged copy of the Q-network to generate the TD target in order to stabilize training. We call this a **target network**. If you are interested to know more about this, I encourage you to read the paper :)\n",
    "\n",
    "Generally, with off-policy methods, one needs to be aware of memory usage since we are storing a lot of information in the replay buffer. This can be problematic especially when the state dimension is high (e.g., images).\n",
    "\n",
    "Let's now take a look at the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size):\n",
    "        # Use deque instead of list so we do not have to manually \n",
    "        # pop the buffer when it reaches max buffer size\n",
    "        self.buffer = deque(maxlen = buffer_size) \n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Function to push the data into buffer.\n",
    "        Input:\n",
    "            - state: state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - action: int\n",
    "            - reward: float\n",
    "            - next_state: next_state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - done: bool\n",
    "        \"\"\"\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Function to sample a batch from replay buffer.\n",
    "        Input:\n",
    "            - batch_size: an int\n",
    "        \"\"\"\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        for i in range(batch_size):\n",
    "            state = samples[i][0]\n",
    "            action = samples[i][1]\n",
    "            reward = samples[i][2]\n",
    "            next_state = samples[i][3]\n",
    "            done = samples[i][4]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards), np.stack(next_states), np.stack(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = F.relu(self.dropout(self.fc1(inp)))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        \n",
    "        # DQN parameters\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.buffer_size = 1000\n",
    "        self.epsilon_init = 1.0\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 200\n",
    "        self.gamma = 0.99\n",
    "        self.log_interval = 250 # print progress per this many episodes\n",
    "        self.target_update_freq = 10 # update target network per this many episodes\n",
    "        \n",
    "        # Models\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict()) # copy of q_network\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.q_network_optimizer = optim.Adam(self.q_network.parameters(), lr = 1e-2)\n",
    "        self.q_network.train()\n",
    "        self.target_network.eval() # We never train the network we use to generate the TD target!\n",
    "\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # [1, dim_space]\n",
    "        # Using epsilon greedy as our policy\n",
    "        if np.random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_value = self.q_network(state) # [1, action_space]\n",
    "                action  = q_value.max(1) # returns both the max values and max index\n",
    "                action = action[1].data[0] # [1] indicates we want the max index\n",
    "                action = action.item()\n",
    "        else: # With probability epsilon, select random action\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Convert ndarray to torch tensor\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        dones = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        # Calculate TD error\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1) # [batch_size]\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach() # [0] indicates we want the max values (not the indices!), detach since this is the target # [batch_size]\n",
    "        td_target = rewards + self.gamma * next_q_values * (1. - dones) \n",
    "        loss = F.mse_loss(q_values, td_target)\n",
    "        \n",
    "        # Update model\n",
    "        self.q_network_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.q_network_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4\n",
    "action_dim = 2\n",
    "agent = DQNAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "running_reward = 10\n",
    "target_update_freq = 10 # update target network every X episode\n",
    "episode = 1 # indicate episode number\n",
    "\n",
    "state, ep_reward = env.reset(), 0\n",
    "for i in range(1, 10000):\n",
    "    \n",
    "    # Update epsilon and pick action\n",
    "    epsilon = agent.epsilon_end + (agent.epsilon_init - agent.epsilon_end) * np.exp(-1. * i / agent.epsilon_decay)\n",
    "    action = agent.get_action(state, epsilon)\n",
    "\n",
    "    # Take a step\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    # Update replay buffer\n",
    "    agent.replay_buffer.update(state, action, reward, next_state, done)\n",
    "\n",
    "    # Once replay buffer size is larger than batch size, start training\n",
    "    if len(agent.replay_buffer.buffer) > batch_size:\n",
    "        agent.update(batch_size)\n",
    "\n",
    "    # Update episode reward and check for end episode\n",
    "    ep_reward += reward\n",
    "    if done: # If episode is done: update running reward, reset env, reset episode reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        episode += 1 # increment episode count (used for updating target network)\n",
    "        state, ep_reward = env.reset(), 0\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    # Occasionally, update the target_network by copying the q_network\n",
    "    if episode % agent.target_update_freq == 0:\n",
    "        agent.target_network.load_state_dict(agent.q_network.state_dict())\n",
    "        \n",
    "    if i % agent.log_interval == 0:\n",
    "        print('Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "    \n",
    "    # Stopping criteria\n",
    "    if running_reward > 100:\n",
    "        print('Solved: Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
