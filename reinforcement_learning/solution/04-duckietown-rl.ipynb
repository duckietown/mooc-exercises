{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m count\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0') # so we can do .to(device)\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:commons:version: 6.2.4 *\n",
      "DEBUG:typing:version: 6.2.3\n",
      "DEBUG:duckietown_world:duckietown-world version 6.2.39 path /usr/local/lib/python3.8/dist-packages\n",
      "DEBUG:geometry:PyGeometry-z6 version 2.1.4 path /usr/local/lib/python3.8/dist-packages\n",
      "DEBUG:aido_schemas:aido-protocols version 6.0.59 path /usr/local/lib/python3.8/dist-packages\n",
      "DEBUG:nodes:version 6.2.13 path /usr/local/lib/python3.8/dist-packages pyparsing 2.4.6\n",
      "DEBUG:gym-duckietown:gym-duckietown version 6.1.30 path /usr/local/lib/python3.8/dist-packages\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'audio': ('xaudio2', 'directsound', 'openal', 'pulse', 'silent'), 'debug_font': False, 'debug_gl': True, 'debug_gl_trace': False, 'debug_gl_trace_args': False, 'debug_gl_shaders': False, 'debug_graphics_batch': False, 'debug_lib': False, 'debug_media': False, 'debug_texture': False, 'debug_trace': False, 'debug_trace_args': False, 'debug_trace_depth': 1, 'debug_trace_flush': True, 'debug_win32': False, 'debug_input': False, 'debug_x11': False, 'shadow_window': True, 'vsync': None, 'xsync': True, 'xlib_fullscreen_override_redirect': False, 'search_local_libs': True, 'win32_gdi_font': False, 'headless': False, 'headless_device': 0, 'win32_disable_shaping': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libEGL warning: failed to open /dev/dri/renderD129: Permission denied\n",
      "\n",
      "libEGL warning: failed to open /dev/dri/renderD129: Permission denied\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown OpenGL API: None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym_duckietown\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Simulator\n\u001b[1;32m      4\u001b[0m env \u001b[38;5;241m=\u001b[39m Simulator()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym_duckietown/simulator.py:44\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logger\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheck_hw\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_graphics_information\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     45\u001b[0m     agent_boundbox,\n\u001b[1;32m     46\u001b[0m     generate_norm,\n\u001b[1;32m     47\u001b[0m     intersects,\n\u001b[1;32m     48\u001b[0m     safety_circle_intersection,\n\u001b[1;32m     49\u001b[0m     safety_circle_overlap,\n\u001b[1;32m     50\u001b[0m     tile_corners,\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistortion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Distortion\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InvalidMapException, NotInLane\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym_duckietown/collision.py:6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraphics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rotate_point\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magent_boundbox\u001b[39m(true_pos, width, length, f_vec, r_vec):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    Compute bounding box for agent using its dimensions,\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    current position, and angle of rotation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    1 - 2\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/gym_duckietown/graphics.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gl\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/image/__init__.py:139\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mopen\u001b[39m, BytesIO\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gl_info\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m asbytes\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/gl/__init__.py:226\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pyglet_doc_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyglet.window\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _sys\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;129;01mand\u001b[39;00m _pyglet\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshadow_window\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;66;03m# trickery is for circular import\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     _pyglet\u001b[38;5;241m.\u001b[39mgl \u001b[38;5;241m=\u001b[39m _sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/window/__init__.py:1915\u001b[0m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pyglet_doc_run:\n\u001b[1;32m   1914\u001b[0m     pyglet\u001b[38;5;241m.\u001b[39mwindow \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m-> 1915\u001b[0m     \u001b[43mgl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_shadow_window\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/gl/__init__.py:200\u001b[0m, in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124;03m\"\"\"Shadow window does not need a projection.\"\"\"\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m _shadow_window \u001b[38;5;241m=\u001b[39m \u001b[43mShadowWindow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m _shadow_window\u001b[38;5;241m.\u001b[39mswitch_to()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyglet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m app\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/gl/__init__.py:194\u001b[0m, in \u001b[0;36m_create_shadow_window.<locals>.ShadowWindow.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/window/__init__.py:558\u001b[0m, in \u001b[0;36mBaseWindow.__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, file_drops, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m template_config \u001b[38;5;129;01min\u001b[39;00m [gl\u001b[38;5;241m.\u001b[39mConfig(double_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, depth_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m, major_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, minor_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m    555\u001b[0m                         gl\u001b[38;5;241m.\u001b[39mConfig(double_buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, depth_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, major_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, minor_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m),\n\u001b[1;32m    556\u001b[0m                         \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 558\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_best_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    560\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m NoSuchConfigException:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/canvas/base.py:191\u001b[0m, in \u001b[0;36mScreen.get_best_config\u001b[0;34m(self, template)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     configs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matching_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configs:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m window\u001b[38;5;241m.\u001b[39mNoSuchConfigException()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/canvas/headless.py:90\u001b[0m, in \u001b[0;36mHeadlessScreen.get_matching_configs\u001b[0;34m(self, template)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_matching_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, template):\n\u001b[1;32m     89\u001b[0m     canvas \u001b[38;5;241m=\u001b[39m HeadlessCanvas(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 90\u001b[0m     configs \u001b[38;5;241m=\u001b[39m \u001b[43mtemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcanvas\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# XXX deprecate\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m config \u001b[38;5;129;01min\u001b[39;00m configs:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyglet/gl/headless.py:77\u001b[0m, in \u001b[0;36mHeadlessConfig.match\u001b[0;34m(self, canvas)\u001b[0m\n\u001b[1;32m     75\u001b[0m     attrs\u001b[38;5;241m.\u001b[39mextend([EGL_RENDERABLE_TYPE, EGL_OPENGL_ES3_BIT])\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown OpenGL API: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopengl_api\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     78\u001b[0m attrs\u001b[38;5;241m.\u001b[39mextend([EGL_NONE])\n\u001b[1;32m     79\u001b[0m attrs_list \u001b[38;5;241m=\u001b[39m (egl\u001b[38;5;241m.\u001b[39mEGLint \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(attrs))(\u001b[38;5;241m*\u001b[39mattrs)\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown OpenGL API: None"
     ]
    }
   ],
   "source": [
    "from gym_duckietown.simulator import Simulator\n",
    "\n",
    "\n",
    "env = Simulator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Networks (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN is an off-policy Q learning method. Unlike REINFORCE, DQN learns a Q networks (as opposed to policy) and can use trajectories that the agent has collected in the past to train the current Q network by storing them in a replay buffer. The learning objective in DQN is also different than in REINFORCE: DQN tries to minimize the **Temporal Difference (TD)** error (i.e., difference between prediction of Q values and the TD target). We are not going to talk about TD learning in detail here, but if you are especially interested to work with RL, I recommend you to look into TD learning and SARSA to have a better understanding about DQN. The TD target for DQN is defined as:\n",
    "\n",
    "$$\n",
    "Q_{target}(s,a) = r + \\gamma \\max_{a'}Q(s',a')\n",
    "$$\n",
    "\n",
    "The DQN algorithm is shown below:\n",
    "\n",
    "<figure>\n",
    "  <div style=\"text-align:center;\">\n",
    "  <img src=\"assets/05/dqn_algo.png\", width = 600>\n",
    "  <figcaption>Source: Mnih et al. (2013). Playing Atari with Deep Reinforcement Learning.</figcaption>\n",
    "  </div>\n",
    "</figure>\n",
    "\n",
    "Note that depending on the use case, we may or may not use the feature network $\\phi$ shown in the algorithm above. For this example, we know what the state is, so we do not need to differentiate between observations $x$ and states $s$.\n",
    "\n",
    "In practice, we often use another network that is a lagged copy of the Q-network to generate the TD target in order to stabilize training. We call this a **target network**. If you are interested to know more about this, I encourage you to read the paper :)\n",
    "\n",
    "Generally, with off-policy methods, one needs to be aware of memory usage since we are storing a lot of information in the replay buffer. This can be problematic especially when the state dimension is high (e.g., images).\n",
    "\n",
    "Let's now take a look at the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_size):\n",
    "        # Use deque instead of list so we do not have to manually \n",
    "        # pop the buffer when it reaches max buffer size\n",
    "        self.buffer = deque(maxlen = buffer_size) \n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Function to push the data into buffer.\n",
    "        Input:\n",
    "            - state: state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - action: int\n",
    "            - reward: float\n",
    "            - next_state: next_state ndarray [state_dim (e.g., H x W x 3)]\n",
    "            - done: bool\n",
    "        \"\"\"\n",
    "        self.buffer.append([state, action, reward, next_state, done])\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Function to sample a batch from replay buffer.\n",
    "        Input:\n",
    "            - batch_size: an int\n",
    "        \"\"\"\n",
    "        samples = random.sample(self.buffer, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        for i in range(batch_size):\n",
    "            state = samples[i][0]\n",
    "            action = samples[i][1]\n",
    "            reward = samples[i][2]\n",
    "            next_state = samples[i][3]\n",
    "            done = samples[i][4]\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards), np.stack(next_states), np.stack(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x = F.relu(self.dropout(self.fc1(inp)))\n",
    "        out = self.fc2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        \n",
    "        # DQN parameters\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.buffer_size = 1000\n",
    "        self.epsilon_init = 1.0\n",
    "        self.epsilon_end = 0.05\n",
    "        self.epsilon_decay = 200\n",
    "        self.gamma = 0.99\n",
    "        self.log_interval = 250 # print progress per this many episodes\n",
    "        self.target_update_freq = 10 # update target network per this many episodes\n",
    "        \n",
    "        # Models\n",
    "        self.q_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict()) # copy of q_network\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.q_network_optimizer = optim.Adam(self.q_network.parameters(), lr = 1e-2)\n",
    "        self.q_network.train()\n",
    "        self.target_network.eval() # We never train the network we use to generate the TD target!\n",
    "\n",
    "\n",
    "    def get_action(self, state, epsilon):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device) # [1, dim_space]\n",
    "        # Using epsilon greedy as our policy\n",
    "        if np.random.random() > epsilon:\n",
    "            with torch.no_grad():\n",
    "                q_value = self.q_network(state) # [1, action_space]\n",
    "                action  = q_value.max(1) # returns both the max values and max index\n",
    "                action = action[1].data[0] # [1] indicates we want the max index\n",
    "                action = action.item()\n",
    "        else: # With probability epsilon, select random action\n",
    "            action = random.randrange(self.action_dim)\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def update(self, batch_size):\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
    "        \n",
    "        # Convert ndarray to torch tensor\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        next_states = torch.from_numpy(next_states).float().to(device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(device)\n",
    "        actions = torch.from_numpy(actions).long().to(device)\n",
    "        dones = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        # Calculate TD error\n",
    "        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1) # [batch_size]\n",
    "        next_q_values = self.target_network(next_states).max(1)[0].detach() # [0] indicates we want the max values (not the indices!), detach since this is the target # [batch_size]\n",
    "        td_target = rewards + self.gamma * next_q_values * (1. - dones) \n",
    "        loss = F.mse_loss(q_values, td_target)\n",
    "        \n",
    "        # Update model\n",
    "        self.q_network_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.q_network_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start training the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 4\n",
    "action_dim = 2\n",
    "agent = DQNAgent(state_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "running_reward = 10\n",
    "target_update_freq = 10 # update target network every X episode\n",
    "episode = 1 # indicate episode number\n",
    "\n",
    "state, ep_reward = env.reset(), 0\n",
    "for i in range(1, 10000):\n",
    "    \n",
    "    # Update epsilon and pick action\n",
    "    epsilon = agent.epsilon_end + (agent.epsilon_init - agent.epsilon_end) * np.exp(-1. * i / agent.epsilon_decay)\n",
    "    action = agent.get_action(state, epsilon)\n",
    "\n",
    "    # Take a step\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "    # Update replay buffer\n",
    "    agent.replay_buffer.update(state, action, reward, next_state, done)\n",
    "\n",
    "    # Once replay buffer size is larger than batch size, start training\n",
    "    if len(agent.replay_buffer.buffer) > batch_size:\n",
    "        agent.update(batch_size)\n",
    "\n",
    "    # Update episode reward and check for end episode\n",
    "    ep_reward += reward\n",
    "    if done: # If episode is done: update running reward, reset env, reset episode reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        episode += 1 # increment episode count (used for updating target network)\n",
    "        state, ep_reward = env.reset(), 0\n",
    "    else:\n",
    "        state = next_state\n",
    "\n",
    "    # Occasionally, update the target_network by copying the q_network\n",
    "    if episode % agent.target_update_freq == 0:\n",
    "        agent.target_network.load_state_dict(agent.q_network.state_dict())\n",
    "        \n",
    "    if i % agent.log_interval == 0:\n",
    "        print('Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "    \n",
    "    # Stopping criteria\n",
    "    if running_reward > 100:\n",
    "        print('Solved: Episode %d \\t Running Reward: %.2f' \n",
    "              % (i, running_reward))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
